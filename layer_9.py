# -*- coding: utf-8 -*-
"""Layer_9.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Oi7C7PQcpp2Fm470gayzKDIfuxHbG_cT
"""

import pandas as pd
import numpy as np

#Constants
L1 = 'label_1'
L2 = 'label_2'
L3 = 'label_3'
L4 = 'label_4'
LABELS = [L1, L2, L3, L4]
AGE_LABEL = L2
FEATURES =  [f'feature_{i}' for i in range(1,769)]

train_df = pd.read_csv("train_9.csv")
valid_df = pd.read_csv("valid_9.csv")
test_df = pd.read_csv("test_9.csv")

from pandas.core.arrays.period import validate_dtype_freq
# Scaling the data
from sklearn.preprocessing import RobustScaler # RobustScaler

x_train = {}
x_valid = {}
x_test = {}
y_train = {}
y_valid = {}

for target_label in LABELS:
  tr_df = train_df[train_df['label_2'].notna()] if target_label == 'label_2' else train_df # remove NA values in label 2
  vl_df = valid_df[valid_df['label_2'].notna()] if target_label == 'label_2' else valid_df
  t_df = test_df
  scaler = RobustScaler()
  x_train[target_label] = pd.DataFrame(scaler.fit_transform(tr_df.drop(LABELS, axis=1)), columns = FEATURES)
  x_test[target_label] = pd.DataFrame(scaler.fit_transform(t_df.drop("ID", axis=1)), columns = FEATURES)
  y_train[target_label] = tr_df[target_label]
  x_valid[target_label] = pd.DataFrame(scaler.transform(vl_df.drop(LABELS, axis=1)), columns = FEATURES)
  y_valid[target_label] = vl_df[target_label]

print(x_test)

"""**Model Training Function**"""

from sklearn import svm
from sklearn import metrics
from sklearn.ensemble import RandomForestClassifier

def classificationModelsAccuracy(model, x_train, y_train, x_valid, y_valid):
  models = ['svm', 'xgboost']
  created_model = 0
  if 'svm' == model:
    created_model = svm.SVC(kernel='linear', class_weight='balanced')
    created_model.fit(x_train, y_train)
    y_pred = created_model.predict(x_valid)
    print('SVM accuracy Score :', metrics.accuracy_score(y_valid, y_pred))
  elif 'randomForest' == model:
    created_model = RandomForestClassifier(n_estimators=100, random_state=42)
    created_model.fit(x_train, y_train)
    y_pred = created_model.predict(x_valid)
    print('Random Forest accuracy :', metrics.accuracy_score(y_valid, y_pred))
  return created_model

"""--------------------------------------"""

svm = classificationModelsAccuracy('svm', x_train[L1], y_train[L1], x_valid[L1], y_valid[L1])

random_forest = classificationModelsAccuracy('randomForest', x_train[L1], y_train[L1], x_valid[L1], y_valid[L1])

"""**Feature Reduction**

Select K-Best
"""

from sklearn.feature_selection import SelectKBest, f_classif

def selectKBestFunction(x_train, y_train, x_valid, x_test, k):
  selector = SelectKBest(score_func=f_classif, k=k)
  x_new_train = selector.fit_transform(x_train, y_train)
  x_new_valid = selector.transform(x_valid)
  x_new_test = selector.transform(x_test)
  return x_new_train, x_new_valid, x_new_test

"""PCA"""

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

def PCAFunction(L, xTrain, xValid, xTest, n):
    scaler = StandardScaler()

    # Standardize the data
    x_train_scaled = pd.DataFrame(scaler.fit_transform(xTrain), columns=xTrain.columns)
    x_valid_scaled = pd.DataFrame(scaler.transform(xValid), columns=xValid.columns)
    x_test_scaled = pd.DataFrame(scaler.transform(xTest), columns=xTest.columns)

    pca = PCA(n_components=n, svd_solver='full')
    pca.fit(x_train_scaled)
    # Transform the data and convert the result to DataFrames
    x_pca_train = pd.DataFrame(pca.transform(x_train_scaled))
    x_pca_valid = pd.DataFrame(pca.transform(x_valid_scaled))
    x_pca_test = pd.DataFrame(pca.transform(x_test_scaled))

    return x_pca_train, x_pca_valid, x_pca_test

"""**Hyper Parameter Tuning**

Random Search
"""

from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform, randint

def randomSearch(model, xTrain, yTrain, L):
  param_dist = {
    'C': uniform(0.1, 10),          # Continuous distribution for 'C'
    'kernel': ['linear', 'rbf'],    # List of choices for 'kernel'
    'gamma': uniform(0.001, 1.0)   # Continuous distribution for 'gamma'
  }
  random_search = RandomizedSearchCV(
    estimator=model,
    param_distributions=param_dist,
    n_iter=5,     # Number of random combinations to try
    cv=2,           # Number of cross-validation folds
    n_jobs=-1,     # Use all available CPU cores
    random_state=42
  )

  random_search.fit(xTrain, yTrain)
  best_params = random_search.best_params_
  best_model = random_search.best_estimator_
  return best_model

"""**Cross Validation**"""

from sklearn.model_selection import cross_val_score

def crossValidationFunction(model, x_train, y_train, k):
    scores = cross_val_score(model, x_train, y_train, cv=k)
    print("Cross-validation scores: ", scores)
    print("%0.2f accuracy with a standard deviation of %f" % (scores.mean(), scores.std()))

"""**For Label 1**"""

import matplotlib.pyplot as plt
import seaborn as sn

plt.figure(figsize=(15,6))
sn.countplot(data=y_train, x=L1, color='red')

# without feature engineering
svm = classificationModelsAccuracy('svm', x_train[L1], y_train[L1], x_valid[L1], y_valid[L1])

y_pred_val_label1 = svm.predict(x_valid[L1])
print(y_pred_val_label1.shape)
print(y_valid[L4].shape)
print('SVM accuracy Score :', metrics.accuracy_score(y_valid[L1], y_pred_val_label1))

import csv

array = y_pred_val_label1

# Define the number of rows (assuming all arrays have the same length)
num_rows = len(y_pred_val_label1)

# Create and open a CSV file for writing
with open('layer_9_label_1.csv', mode='w', newline='') as file:
    writer = csv.writer(file)

    # Write data rows
    for i in range(num_rows):
        row = [array[i]]
        writer.writerow(row)

print("CSV file 'layer_9_label_1.csv' has been created.")

"""Applying PCA"""

# Applying PCA
x_pca_train1, x_pca_valid1, x_pca_test1 = PCAFunction(L1, x_train[L1], x_valid[L1], x_test[L1], 0.95)

x_pca_train1.shape
x_pca_valid1.shape
x_pca_test1.shape

from sklearn.svm import SVC

svm_classifier = SVC()
tuned_model1 = randomSearch(svm_classifier, x_pca_train1, y_train[L1], L1)

y_pred1 = tuned_model1.predict(x_pca_valid1)
print(y_pred1.shape)
print(y_valid[L1].shape)
print('After Random Search using SVM accuracy Score :', metrics.accuracy_score(y_valid[L1], y_pred1))

"""Applying K-Best"""

# Applying selectKBest
x_training_label1, x_valid_label1, x_test_label1 = selectKBestFunction(x_train[L1], y_train[L1], x_valid[L1],x_test[L1], 500)

from sklearn.svm import SVC

svm_classifier = SVC()
tuned_model1_2 = randomSearch(svm_classifier, x_training_label1, y_train[L1], L1)

y_pred = tuned_model1_2.predict(x_valid_label1)
print(y_pred.shape)
print(y_valid[L1].shape)
print('After Random Search using SVM accuracy Score :', metrics.accuracy_score(y_valid[L1], y_pred))

# cross validation
crossValidationFunction(tuned_model1_2, x_training_label1, y_train[L1], 3)

"""**For Label 2**"""

import matplotlib.pyplot as plt
import seaborn as sn

plt.figure(figsize=(15,6))
sn.countplot(data=y_train, x=L2, color='blue')

# Without feature engineering
svm = classificationModelsAccuracy('svm', x_train[L2], y_train[L2], x_valid[L2], y_valid[L2])

"""Applying PCA"""

# Applying PCA
x_pca_train2, x_pca_valid2, x_pca_test2 = PCAFunction(L2, x_train[L2], x_valid[L2], x_test[L2], 0.97)

x_pca_train2.shape

from sklearn.svm import SVC

svm_classifier = SVC(class_weight='balanced')
tuned_model2 = randomSearch(svm_classifier, x_pca_train2, y_train[L2], L2)

y_pred = tuned_model2.predict(x_pca_valid2)
print(y_pred.shape)
print(y_valid[L2].shape)
print('After Random Search using SVM accuracy Score :', metrics.accuracy_score(y_valid[L2], y_pred))

"""Applying K-Best"""

x_training_label2, x_valid_label2, x_test_label2 = selectKBestFunction(x_train[L2], y_train[L2], x_valid[L2], x_test[L2], 500)

from sklearn.svm import SVC

svm_classifier = SVC(class_weight='balanced')
tuned_model2_2 = randomSearch(svm_classifier, x_training_label2, y_train[L2], L2)

y_pred_val_label2 = tuned_model2_2.predict(x_valid_label2)
print(y_pred_val_label2.shape)
print(y_valid[L2].shape)
print('After Random Search using SVM accuracy Score :', metrics.accuracy_score(y_valid[L2], y_pred_val_label2))

import csv

array = y_pred_val_label2

# Define the number of rows (assuming all arrays have the same length)
num_rows = len(y_pred_val_label2)

# Create and open a CSV file for writing
with open('layer_9_label_2.csv', mode='w', newline='') as file:
    writer = csv.writer(file)

    # Write data rows
    for i in range(num_rows):
        row = [array[i]]
        writer.writerow(row)

print("CSV file 'layer_9_label_2.csv' has been created.")

"""**For Label 3**"""

import matplotlib.pyplot as plt
import seaborn as sn

plt.figure(figsize=(15,6))
sn.countplot(data=y_train, x=L3, color='green')

svm = classificationModelsAccuracy('svm', x_train[L3], y_train[L3], x_valid[L3], y_valid[L3])

"""Applying PCA"""

# Applying PCA
x_pca_train3, x_pca_valid3, x_pca_test3 = PCAFunction(L3, x_train[L3], x_valid[L3], x_test[L3], 0.95)

x_pca_train3.shape

from sklearn.svm import SVC

svm_classifier = SVC()
tuned_model3 = randomSearch(svm_classifier, x_pca_train3, y_train[L3], L3)

y_pred_val_label3 = tuned_model3.predict(x_pca_valid3)
print(y_pred_val_label3.shape)
print(y_valid[L3].shape)
print('After Random Search using SVM accuracy Score :', metrics.accuracy_score(y_valid[L3], y_pred_val_label3))

import csv

array = y_pred_val_label3

# Define the number of rows (assuming all arrays have the same length)
num_rows = len(y_pred_val_label3)

# Create and open a CSV file for writing
with open('layer_9_label_3.csv', mode='w', newline='') as file:
    writer = csv.writer(file)

    # Write data rows
    for i in range(num_rows):
        row = [array[i]]
        writer.writerow(row)

print("CSV file 'layer_9_label_3.csv' has been created.")

#cross validation
crossValidationFunction(tuned_model3, x_pca_train3, y_train[L3], 3)

"""Applying K-Best"""

x_training_label3, x_valid_label3, x_test_label3 = selectKBestFunction(x_train[L3], y_train[L3], x_valid[L3], x_test[L3], 500)

from sklearn.svm import SVC

svm_classifier = SVC(class_weight='balanced')
tuned_model3_2 = randomSearch(svm_classifier, x_training_label3, y_train[L3], L3)

y_pred = tuned_model3_2.predict(x_valid_label3)
print(y_pred.shape)
print(y_valid[L3].shape)
print('After Random Search using SVM accuracy Score :', metrics.accuracy_score(y_valid[L3], y_pred))

"""**For Label 4**"""

import matplotlib.pyplot as plt
import seaborn as sn

plt.figure(figsize=(15,6))
sn.countplot(data=y_train, x=L4, color='orange')

svm = classificationModelsAccuracy('svm', x_train[L4], y_train[L4], x_valid[L4], y_valid[L4])

"""Applying PCA"""

# Applying PCA
x_pca_train4, x_pca_valid4, x_pca_test4 = PCAFunction(L4, x_train[L4], x_valid[L4], x_test[L4], 0.95)

x_pca_train4.shape

from sklearn.svm import SVC

svm_classifier = SVC()
tuned_model4 = randomSearch(svm_classifier, x_pca_train4, y_train[L4], L4)

y_pred_val_label4 = tuned_model4.predict(x_pca_valid4)
print(y_pred_val_label4.shape)
print(y_valid[L4].shape)
print('After Random Search using SVM accuracy Score :', metrics.accuracy_score(y_valid[L4], y_pred_val_label4))

import csv

array = y_pred_val_label4

# Define the number of rows (assuming all arrays have the same length)
num_rows = len(y_pred_val_label4)

# Create and open a CSV file for writing
with open('layer_9_label_4.csv', mode='w', newline='') as file:
    writer = csv.writer(file)

    # Write data rows
    for i in range(num_rows):
        row = [array[i]]
        writer.writerow(row)

print("CSV file 'layer_9_label_4.csv' has been created.")

#cross validation
crossValidationFunction(tuned_model4, x_pca_train4, y_train[L4], 3)

"""Applying K-Best"""

x_training_label4, x_valid_label4, x_test_label4 = selectKBestFunction(x_train[L4], y_train[L4], x_valid[L4],x_test[L4], 500)

from sklearn.svm import SVC

svm_classifier = SVC(class_weight='balanced')
tuned_model4_2 = randomSearch(svm_classifier, x_training_label4, y_train[L4], L4)

y_pred = tuned_model4_2.predict(x_valid_label4)
print(y_pred.shape)
print(y_valid[L4].shape)
print('After Random Search using SVM accuracy Score :', metrics.accuracy_score(y_valid[L4], y_pred))

"""**Predicting the test data**

Label 1
"""

y_pred_label1 = tuned_model1_2.predict(x_test_label1)

type(y_pred_label1)

"""Label 2"""

y_pred_label2 = tuned_model2.predict(x_pca_test2)

"""Label 3"""

y_pred_label3 = tuned_model3.predict(x_pca_test3)

"""Label 4"""

y_pred_label4 = tuned_model4.predict(x_pca_test4)

"""Creating CSV file"""

import csv
import numpy as np

# Create your NumPy arrays (replace these with your actual data)
array1 = y_pred_label1
array2 = y_pred_label2
array3 = y_pred_label3
array4 = y_pred_label4

# Define the number of rows (assuming all arrays have the same length)
num_rows = len(array1)

# Create and open a CSV file for writing
with open('output.csv', mode='w', newline='') as file:
    writer = csv.writer(file)

    # Write the header row
    header = ["ID", "label_1", "label_2", "label_3", "label_4"]
    writer.writerow(header)

    # Write data rows
    for i in range(num_rows):
        row = [i+1, array1[i], array2[i], array3[i], array4[i]]
        writer.writerow(row)

print("CSV file 'output.csv' has been created.")